# Jaewook (Jake) Lee  
140 Governors Drive, Amherst, MA, 01002  
[LinkedIn](https://www.linkedin.com/in/jaewook-lee-67791918b/) | [Google Scholar](https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=DpIL06kAAAAJ) | 
[jaewooklee@cs.umass.edu](mailto:jaewooklee@cs.umass.edu)

---

## EMNLP 2024 Findings - 1107
**Session**: NLP Applications 2  
**Date & Time**: Wednesday, Nov 13, 10:30-12:00  
**Location**: Riverfront Hall  

### Title: 
Exploring Automated Keyword Mnemonics Generation with Large Language Models via Overgenerate-and-Rank

#### Abstract: 
In this paper, we study an under-explored area of language and vocabulary learning: keyword mnemonics, a technique for memorizing vocabulary through memorable associations with a target word via a verbal cue. Typically, creating verbal cues requires extensive human effort and is quite time-consuming, necessitating an automated method that is more scalable. We propose a novel overgenerate-and-rank method via prompting large language models (LLMs) to generate verbal cues and then ranking them according to psycholinguistic measures and takeaways from a pilot user study. To assess cue quality, we conduct both an automated evaluation of imageability and coherence, as well as a human evaluation involving English teachers and learners. Results show that LLM-generated mnemonics are comparable to human-generated ones in terms of imageability, coherence, and perceived usefulness, but there remains plenty of room for improvement due to the diversity in background and preference among language learners.

[Link to Paper](https://aclanthology.org/2024.findings-emnlp.316.pdf)
[Link to Supplementary Materials](./emnlp2024.html)

---

## About Me
Jake is a Ph.D. student in Computer Science at the University of Massachusetts, Amherst, applying generative AI and large language models (LLMs) to solve real-world problems. He excels in designing experiments, developing methods, and building interfaces for human evaluation, leading to publications in top conferences like EMNLP and NAACL. He also has a strong background in computer systems and deep learning frameworks, having developed device-framework interfaces and optimized inference performance through profiling and scheduling algorithms, published in ACM TODAES.

---

## Education  

**University of Massachusetts, Amherst, MA, United States**  
Ph.D. in Computer Science  
GPA: 3.91/4.0  
Advisor: Prof. Andrew Lan  
2022.9 - present  

**Korea University, Seoul, Republic of Korea**  
M.E. in Electrical and Computer Engineering  
GPA: 4.28/4.5  
Advisor: Prof. Seon Wook Kim  
2019.1 - 2022.2  
- Thesis: *Profile-based Optimal Model Partitioning on PIM-based Heterogeneous Platform for Deep Learning Inference*

B.E. in Electrical Engineering, Graduated with honors  
GPA: 3.98/4.5  
2013.3 - 2019.8  
- Two years of mandatory military service (2015.4 - 2017.1)

---

## Research Experience  

### Ph.D. Research in Generative AI and NLP  
**University of Massachusetts, Amherst**  
2022.9 - present  
- **Keyword Mnemonics Generation Using LLMs (EMNLP 2024)**  
Developed an LLM-based pipeline to generate cognitive psychology-based mnemonic aids for vocabulary learning. Designed and ran human evaluations and automated assessments to benchmark LLM-generated mnemonics against manually created ones.  
- **Automated Distractor Generation in Math MCQs (NAACL 2024)**  
Explored the use of in-context learning with LLMs for generating distractors in math multiple-choice questions. Conducted experiments to evaluate both model and educator performance.

### Deep Learning and Processing-in-Memory (PIM) Research  
**Korea University, Seoul**  
2019.1 - 2022.2  
- **Optimized Deep Learning Inference on PIM Platforms (ACM TODAES 2024)**  
Developed an ONNX Runtime interface for PIM platforms, optimizing DNN partitioning on both x86 and ARM architectures. Designed profiling and scheduling algorithms to improve inference performance on heterogeneous platforms.

---

## Publications  
(* denotes equal contribution)

- **Jaewook Lee**, Hunter McNichols, Andrew Lan, ‚ÄúExploring Automated Keyword Mnemonics Generation with Large Language Models via Overgenerate-and-Rank,‚Äù *Findings of the Association for Computational Linguistics: EMNLP, Florida, USA*, 2024 üìé  
- Wanyong Feng\*, **Jaewook Lee**\*, Hunter McNichols\*, Alexander Scarlatos\*, Digory Smith, Simon Woodhead, Nancy Otero Ornelas, Andrew Lan, ‚ÄúExploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models,‚Äù *Findings of the Association for Computational Linguistics: NAACL, Mexico City, Mexico*, 2024 üìé  
- Seok Young Kim\*, **Jaewook Lee**\*, Yoonah Paik, Chang Hyun Kim, Won Jun Lee, Seon Wook Kim, ‚ÄúOptimal Model Partitioning with Low-Overhead Profiling on the PIM-based Platform for Deep Learning Inference,‚Äù *ACM Transactions on Design Automation of Electronic Systems, 29(2)*, 1-22 üìé  
- **Jaewook Lee**, Andrew Lan, ‚ÄúSmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues,‚Äù *International Conference on Artificial Intelligence in Education (AIED), Tokyo, Japan*, 2023 üìé

---

## Posters  

- **Jaewook Lee**, Digory Smith, Simon Woodhead, Andrew Lan, ‚ÄúMath Multiple Choice Question Generation via Human-Large Language Model Collaboration,‚Äù *International Conference on Educational Data Mining (EDM), Atlanta, USA*, 2024 üìé  
- Hunter McNichols, **Jaewook Lee**, Stephen Fancsali, Steve Ritter, Andrew Lan, ‚ÄúCan Large Language Models Replicate ITS Feedback on Open-Ended Math Questions?‚Äù, *International Conference on Educational Data Mining (EDM), Atlanta, USA*, 2024 üìé  
- Nischal Ashok Kumar, Wanyong Feng, **Jaewook Lee**, William Hunter McNichols, and Andrew Lan, ‚ÄúA Conceptual Model for End-to-End Causal Discovery in Knowledge Tracing,‚Äù *International Conference on Educational Data Mining (EDM), Bengaluru, India*, 2023 üìé  

---

## Awards  

- **NAEP Math Automated Scoring Challenge Grand Prize** üèÜ  
*Organized by National Center for Education Statistics (NCES)*  
2023  

- **NeurIPS 2022 Causal Edu Competition (Task 3) - 3rd**  
*Organized by EEDI*  
2022  

- **iOS Application Hackathon Grand Prize** üèÜ  
*Organized by Software Technology and Enterprise, Korea University*  
2018  

---

## Relevant Coursework  

Machine Learning (CMPSCI 689), Natural Language Processing (CMPSCI 685), Reinforcement Learning (CMPSCI 687), Advanced Algorithms (CMPSCI 611), Probabilistic Graphical Models (CMPSCI 688) [Current Semester], Advanced Methods in HCI (CMPSCI 625) [Next Semester]

---

## Skill Set  

- **Deep Learning**: Skilled in PyTorch and ONNX Runtime for model deployment  
- **Debugging & Analysis**: Strong skills in debugging and performance optimization  
- **Programming**: Expertise in C, C++, and Python  
- **Web Development**: Expertise in building interfaces with React for human evaluations  
- **UX/UI**: Knowledge in UX research and interface design  
- **Mobile Development**: Hands-on experience with iOS and Android app development  

--- 

*(Updated: 2024.10)*
