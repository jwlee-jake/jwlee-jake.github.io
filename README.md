# Jaewook (Jake) Lee  
140 Governors Drive, Amherst, MA, 01002  
[LinkedIn](https://www.linkedin.com/in/jaewook-lee-67791918b/) | [Google Scholar](https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=DpIL06kAAAAJ) | jaewooklee@cs.umass.edu  

---

## EMNLP 2024 Findings
**Session**: NLP Applications 2  
**Date & Time**: Wednesday, Nov 13, 10:30-12:00  
**Location**: Riverfront Hall  

### Poster Title
**Exploring Automated Keyword Mnemonics Generation with Large Language Models via Overgenerate-and-Rank**

| Word       | LLM-generated                                                                                      | Human-authored                                                                      |
|------------|----------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|
| abstemious | *Ab*'s diet of only the *stem* of vegetables taught *me* and *us* to be more **abstemious**.       | *Ab*'s *steamy* with *us* when he tells us to be **abstemious**.                    |
| accolade   | The *echo* of the *lad*'s music earned him an **accolade**.                                        | *A* *coal* *aide* receives an **accolade**.                                        |
| aesthetic  | *As* *the* clock *ticked*, he was captivated by the **aesthetic** appeal of the painting.          | Oh beautiful *S*! *That* *tick* tickles the **aesthetic** sense!                    |
| appease    | He cooks *a* dish with *peas* to **appease** her anger.                                            | Tom was **appeasing** a pot *o'* *peas*.                                           |
| archaic    | The twins wonder " *are* these *cake* instructions?" from an **archaic** recipe book.              | Sick kangaroos ride on **archaic** *Ark* *K* - "*Ick!*" is all they can say.       |
| artisan    | In the *art*-loving town, the **artisan** sips his *tea* under the *sun*.                          | "*Art* *is* *sin*," says the Puritan to the **artisan**.                           |

*Table 1: Verbal cues used for human evaluation. Keywords are represented in *italic*, while a target word is in **bold**. \* indicates an anomaly in verbal cue generation using LLM.*

---

## Summary  
Jake is a Ph.D. student in Computer Science at the University of Massachusetts, Amherst, specializing in generative AI and large language models (LLMs) with a focus on education. He excels in designing experiments, developing methods, and building interfaces for human evaluation, leading to publications in top conferences like EMNLP and NAACL. He also has a strong background in computer systems and deep learning frameworks, having developed device-framework interfaces and optimized inference performance through profiling and scheduling algorithms, published in ACM TODAES.

---

## Education  

**University of Massachusetts, Amherst, MA, United States**  
Ph.D. in Computer Science  
GPA: 3.91/4.0  
Advisor: Prof. Andrew Lan  
2022.9 - present  

**Korea University, Seoul, Republic of Korea**  
M.E. in Electrical and Computer Engineering  
GPA: 4.28/4.5  
Advisor: Prof. Seon Wook Kim  
2019.1 - 2022.2  
- Thesis: *Profile-based Optimal Model Partitioning on PIM-based Heterogeneous Platform for Deep Learning Inference*

B.E. in Electrical Engineering, Graduated with honors  
GPA: 3.98/4.5  
2013.3 - 2019.8  
- Two years of mandatory military service (2015.4 - 2017.1)

---

## Research Experience  

### Ph.D. Research in Generative AI and NLP  
**University of Massachusetts, Amherst**  
2022.9 - present  
- **Keyword Mnemonics Generation Using LLMs (EMNLP 2024)**  
Developed an LLM-based pipeline to generate cognitive psychology-based mnemonic aids for vocabulary learning. Designed and ran human evaluations and automated assessments to benchmark LLM-generated mnemonics against manually created ones.  
- **Automated Distractor Generation in Math MCQs (NAACL 2024)**  
Explored the use of in-context learning with LLMs for generating distractors in math multiple-choice questions. Conducted experiments to evaluate both model and educator performance.

### Deep Learning and Processing-in-Memory (PIM) Research  
**Korea University, Seoul**  
2019.1 - 2022.2  
- **Optimized Deep Learning Inference on PIM Platforms (ACM TODAES 2024)**  
Developed an ONNX Runtime interface for PIM platforms, optimizing DNN partitioning on both x86 and ARM architectures. Designed profiling and scheduling algorithms to improve inference performance on heterogeneous platforms.

---

## Publications  
(* denotes equal contribution)

- **Jaewook Lee**, Hunter McNichols, Andrew Lan, ‚ÄúExploring Automated Keyword Mnemonics Generation with Large Language Models via Overgenerate-and-Rank,‚Äù *Findings of the Association for Computational Linguistics: EMNLP, Florida, USA*, 2024 üìé  
- Wanyong Feng\*, **Jaewook Lee**\*, Hunter McNichols\*, Alexander Scarlatos\*, Digory Smith, Simon Woodhead, Nancy Otero Ornelas, Andrew Lan, ‚ÄúExploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models,‚Äù *Findings of the Association for Computational Linguistics: NAACL, Mexico City, Mexico*, 2024 üìé  
- Seok Young Kim\*, **Jaewook Lee**\*, Yoonah Paik, Chang Hyun Kim, Won Jun Lee, Seon Wook Kim, ‚ÄúOptimal Model Partitioning with Low-Overhead Profiling on the PIM-based Platform for Deep Learning Inference,‚Äù *ACM Transactions on Design Automation of Electronic Systems, 29(2)*, 1-22 üìé  
- **Jaewook Lee**, Andrew Lan, ‚ÄúSmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues,‚Äù *International Conference on Artificial Intelligence in Education (AIED), Tokyo, Japan*, 2023 üìé

---

## Posters  

- **Jaewook Lee**, Digory Smith, Simon Woodhead, Andrew Lan, ‚ÄúMath Multiple Choice Question Generation via Human-Large Language Model Collaboration,‚Äù *International Conference on Educational Data Mining (EDM), Atlanta, USA*, 2024 üìé  
- Hunter McNichols, **Jaewook Lee**, Stephen Fancsali, Steve Ritter, Andrew Lan, ‚ÄúCan Large Language Models Replicate ITS Feedback on Open-Ended Math Questions?‚Äù, *International Conference on Educational Data Mining (EDM), Atlanta, USA*, 2024 üìé  
- Nischal Ashok Kumar, Wanyong Feng, **Jaewook Lee**, William Hunter McNichols, and Andrew Lan, ‚ÄúA Conceptual Model for End-to-End Causal Discovery in Knowledge Tracing,‚Äù *International Conference on Educational Data Mining (EDM), Bengaluru, India*, 2023 üìé  

---

## Awards  

- **NAEP Math Automated Scoring Challenge Grand Prize** üèÜ  
*Organized by National Center for Education Statistics (NCES)*  
2023  

- **NeurIPS 2022 Causal Edu Competition (Task 3) - 3rd**  
*Organized by EEDI*  
2022  

- **iOS Application Hackathon Grand Prize** üèÜ  
*Organized by Software Technology and Enterprise, Korea University*  
2018  

---

## Relevant Coursework  

Machine Learning (CMPSCI 689), Natural Language Processing (CMPSCI 685), Reinforcement Learning (CMPSCI 687), Advanced Algorithms (CMPSCI 611), Probabilistic Graphical Models (CMPSCI 688) [Current Semester], Advanced Methods in HCI (CMPSCI 625) [Next Semester]

---

## Skill Set  

- **Deep Learning**: Skilled in PyTorch and ONNX Runtime for model deployment  
- **Debugging & Analysis**: Strong skills in debugging and performance optimization  
- **Programming**: Expertise in C, C++, and Python  
- **Web Development**: Expertise in building interfaces with React for human evaluations  
- **UX/UI**: Knowledge in UX research and interface design  
- **Mobile Development**: Hands-on experience with iOS and Android app development  

--- 

*(Updated: 2024.10)*
